{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5517f68",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# Analyse du Taux d'Attrition des Employ√©s - HumanForYou\n",
    "\n",
    "## Contexte du Projet\n",
    "\n",
    "**Entreprise** : HumanForYou, entreprise pharmaceutique  \n",
    "**Probl√©matique** : Taux d'attrition (turnover) √©lev√© d'environ 15%  \n",
    "**Objectif** : Identifier les facteurs influen√ßant l'attrition et construire un mod√®le pr√©dictif pour aider l'entreprise √† r√©duire son taux de rotation\n",
    "\n",
    "### Enjeux Business\n",
    "\n",
    "L'attrition des employ√©s repr√©sente un co√ªt significatif pour l'entreprise :\n",
    "- **Co√ªts directs** : Recrutement, formation, int√©gration des nouveaux employ√©s\n",
    "- **Co√ªts indirects** : Perte de productivit√©, perte de connaissances, impact sur le moral des √©quipes\n",
    "- **Estimation** : Le co√ªt de remplacement d'un employ√© peut repr√©senter 50% √† 200% de son salaire annuel\n",
    "\n",
    "### Donn√©es Disponibles\n",
    "\n",
    "Nous disposons de 4 fichiers CSV :\n",
    "\n",
    "| Fichier | Description | Variables Cl√©s |\n",
    "|---------|-------------|----------------|\n",
    "| `general_data.csv` | Donn√©es RH principales | Age, Salaire, Anciennet√©, Department, **Attrition** (cible) |\n",
    "| `manager_survey_data.csv` | √âvaluations manag√©riales | JobInvolvement, PerformanceRating |\n",
    "| `employee_survey_data.csv` | Enqu√™te satisfaction | EnvironmentSatisfaction, JobSatisfaction, WorkLifeBalance |\n",
    "| `in_time.csv` / `out_time.csv` | Horaires d'arriv√©e/d√©part (2015) | Timestamps pour 261 jours ouvr√©s |\n",
    "\n",
    "### Plan du Notebook\n",
    "\n",
    "1. **Pr√©paration de l'environnement** - Imports et configuration\n",
    "2. **Chargement et exploration** - Import des 4 fichiers CSV\n",
    "3. **Analyse des valeurs manquantes** - Visualisation avec missingno\n",
    "4. **Fusion des datasets** - Jointure sur EmployeeID\n",
    "5. **Feature Engineering** - M√©triques d'horaires\n",
    "6. **Pipeline de pr√©paration** - Transformation des donn√©es\n",
    "7. **Analyse Exploratoire (EDA)** - Visualisations et corr√©lations\n",
    "8. **Mod√©lisation** - Comparaison multi-mod√®les\n",
    "9. **Optimisation et Interpr√©tabilit√©** - GridSearch et SHAP\n",
    "10. **Recommandations Business** - Actions RH concr√®tes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3729aed",
   "metadata": {},
   "source": [
    "## 1. Pr√©paration de l'Environnement\n",
    "\n",
    "### Pourquoi ces librairies ?\n",
    "\n",
    "| Librairie | Usage | Justification |\n",
    "|-----------|-------|---------------|\n",
    "| `pandas` | Manipulation de donn√©es | Standard pour les DataFrames, lecture CSV, jointures |\n",
    "| `numpy` | Calculs num√©riques | Op√©rations vectoris√©es, g√©n√©ration al√©atoire |\n",
    "| `matplotlib` / `seaborn` | Visualisation | Graphiques statistiques de qualit√© publication |\n",
    "| `plotly` | Visualisation interactive | Graphiques 3D, exploration interactive |\n",
    "| `missingno` | Analyse des NA | Visualisation intuitive des valeurs manquantes |\n",
    "| `sklearn` | Machine Learning | Pipelines, mod√®les, m√©triques standardis√©s |\n",
    "| `imblearn` | D√©s√©quilibre de classes | SMOTE pour sur-√©chantillonnage synth√©tique |\n",
    "| `xgboost` | Gradient Boosting | Algorithme performant pour la classification |\n",
    "| `shap` | Interpr√©tabilit√© | Explication des pr√©dictions (Shapley values) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0077bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (√† ex√©cuter une seule fois)\n",
    "%pip install pandas numpy matplotlib seaborn plotly missingno scikit-learn imbalanced-learn xgboost shap -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f81e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS - Organisation par cat√©gorie (style Workshop)\n",
    "# =============================================================================\n",
    "\n",
    "# Compatibilit√© et configuration\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Manipulation de donn√©es\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualisation statique\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualisation interactive\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Analyse des valeurs manquantes\n",
    "import missingno as msno\n",
    "\n",
    "# Machine Learning - Pr√©paration\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Machine Learning - Mod√®les\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Machine Learning - M√©triques\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, confusion_matrix, classification_report,\n",
    "                             roc_curve, auc)\n",
    "\n",
    "# Gestion du d√©s√©quilibre de classes\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Interpr√©tabilit√©\n",
    "import shap\n",
    "\n",
    "# Configuration de l'affichage (style Workshop)\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "sns.set()\n",
    "sns.set_context('talk')\n",
    "\n",
    "# Configuration pandas\n",
    "pd.set_option('display.max_rows', 30)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 2)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Reproductibilit√© - Graine al√©atoire fixe (comme dans le Workshop)\n",
    "np.random.seed(42)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Chemin vers les donn√©es\n",
    "DATA_PATH = \"data/\"\n",
    "\n",
    "print(\"‚úì Environnement configur√© avec succ√®s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c8468",
   "metadata": {},
   "source": [
    "## 2. Chargement des Donn√©es\n",
    "\n",
    "### Strat√©gie de chargement\n",
    "\n",
    "Pour chaque fichier CSV, nous utilisons `pd.read_csv()` avec des param√®tres sp√©cifiques :\n",
    "\n",
    "- **`na_values=['NA']`** : Convertit les cha√Ænes \"NA\" en `NaN` pandas (particuli√®rement important pour `employee_survey_data.csv`)\n",
    "- **`index_col=0`** : Utilise la premi√®re colonne comme index (pour `in_time.csv` et `out_time.csv`)\n",
    "\n",
    "> **Note** : Les fichiers `in_time.csv` et `out_time.csv` contiennent 262 colonnes (1 index + 261 jours ouvr√©s de 2015). L'index correspond √† l'EmployeeID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc16859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHARGEMENT DES 4 FICHIERS CSV\n",
    "# =============================================================================\n",
    "\n",
    "# 1. Donn√©es RH principales (contient la variable cible Attrition)\n",
    "general_data = pd.read_csv(DATA_PATH + \"general_data.csv\")\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERAL_DATA.CSV - Donn√©es RH principales\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dimensions : {general_data.shape[0]} lignes √ó {general_data.shape[1]} colonnes\")\n",
    "print(f\"\\nColonnes : {list(general_data.columns)}\")\n",
    "\n",
    "# 2. √âvaluations manag√©riales\n",
    "manager_survey = pd.read_csv(DATA_PATH + \"manager_survey_data.csv\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MANAGER_SURVEY_DATA.CSV - √âvaluations manag√©riales\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dimensions : {manager_survey.shape[0]} lignes √ó {manager_survey.shape[1]} colonnes\")\n",
    "print(f\"Colonnes : {list(manager_survey.columns)}\")\n",
    "\n",
    "# 3. Enqu√™te satisfaction employ√©s (avec valeurs \"NA\" √† traiter)\n",
    "# na_values=['NA'] : convertit les cha√Ænes \"NA\" en NaN pandas\n",
    "employee_survey = pd.read_csv(DATA_PATH + \"employee_survey_data.csv\", na_values=['NA'])\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EMPLOYEE_SURVEY_DATA.CSV - Enqu√™te satisfaction\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dimensions : {employee_survey.shape[0]} lignes √ó {employee_survey.shape[1]} colonnes\")\n",
    "print(f\"Colonnes : {list(employee_survey.columns)}\")\n",
    "\n",
    "# 4. Horaires d'arriv√©e et de d√©part\n",
    "# index_col=0 : la premi√®re colonne (index num√©rique) correspond √† EmployeeID\n",
    "in_time = pd.read_csv(DATA_PATH + \"in_time.csv\", index_col=0, na_values=['NA'])\n",
    "out_time = pd.read_csv(DATA_PATH + \"out_time.csv\", index_col=0, na_values=['NA'])\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"IN_TIME.CSV & OUT_TIME.CSV - Horaires (2015)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dimensions in_time : {in_time.shape[0]} lignes √ó {in_time.shape[1]} colonnes\")\n",
    "print(f\"Dimensions out_time : {out_time.shape[0]} lignes √ó {out_time.shape[1]} colonnes\")\n",
    "print(f\"P√©riode couverte : {in_time.columns[0]} √† {in_time.columns[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d2c9b9",
   "metadata": {},
   "source": [
    "### 2.1 Exploration de general_data.csv\n",
    "\n",
    "Ce fichier contient les donn√©es RH principales et la **variable cible `Attrition`** (Yes/No).\n",
    "\n",
    "**Questions √† se poser :**\n",
    "- Quels types de donn√©es avons-nous (num√©riques, cat√©gorielles) ?\n",
    "- Y a-t-il des valeurs manquantes ?\n",
    "- Quelles sont les distributions des variables num√©riques ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ddc676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aper√ßu des premi√®res lignes\n",
    "print(\"Aper√ßu des donn√©es :\")\n",
    "general_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information sur les types de donn√©es et valeurs non-nulles\n",
    "print(\"Structure des donn√©es :\")\n",
    "general_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ef1371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives des variables num√©riques\n",
    "print(\"Statistiques descriptives :\")\n",
    "general_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba80da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des variables cat√©gorielles (comme dans le Workshop EDA)\n",
    "# value_counts() permet de conna√Ætre le nombre de valeurs diff√©rentes\n",
    "print(\"=\" * 60)\n",
    "print(\"VARIABLES CAT√âGORIELLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "categorical_cols = general_data.select_dtypes(include=['object']).columns.tolist()\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(general_data[col].value_counts())\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ca4e66",
   "metadata": {},
   "source": [
    "### 2.2 Variable Cible : Attrition\n",
    "\n",
    "La variable `Attrition` est notre **variable cible** (√† pr√©dire). Elle est binaire :\n",
    "- **Yes** : L'employ√© a quitt√© l'entreprise\n",
    "- **No** : L'employ√© est rest√©\n",
    "\n",
    "**Important** : Nous devons analyser le **d√©s√©quilibre de classes**. Un ratio fortement d√©s√©quilibr√© (ex: 85% No / 15% Yes) n√©cessite des techniques sp√©cifiques pour √©viter que le mod√®le ne pr√©dise toujours la classe majoritaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f59d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de la distribution de la variable cible\n",
    "print(\"=\" * 60)\n",
    "print(\"DISTRIBUTION DE LA VARIABLE CIBLE : ATTRITION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Comptage\n",
    "attrition_counts = general_data['Attrition'].value_counts()\n",
    "attrition_pct = general_data['Attrition'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nComptage :\")\n",
    "print(attrition_counts)\n",
    "print(\"\\nPourcentages :\")\n",
    "print(attrition_pct.round(2))\n",
    "\n",
    "# Calcul du ratio de d√©s√©quilibre\n",
    "ratio = attrition_counts['No'] / attrition_counts['Yes']\n",
    "print(f\"\\nRatio de d√©s√©quilibre (No/Yes) : {ratio:.2f}\")\n",
    "print(f\"‚Üí Il y a {ratio:.1f}x plus d'employ√©s qui restent que d'employ√©s qui partent\")\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Graphique en barres\n",
    "colors = ['#2ecc71', '#e74c3c']  # Vert pour No, Rouge pour Yes\n",
    "ax1 = attrition_counts.plot(kind='bar', ax=axes[0], color=colors, edgecolor='black')\n",
    "axes[0].set_title('Distribution de l\\'Attrition', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Attrition')\n",
    "axes[0].set_ylabel('Nombre d\\'employ√©s')\n",
    "axes[0].set_xticklabels(['Non', 'Oui'], rotation=0)\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for i, (count, pct) in enumerate(zip(attrition_counts, attrition_pct)):\n",
    "    axes[0].text(i, count + 50, f'{count}\\n({pct:.1f}%)', ha='center', fontsize=12)\n",
    "\n",
    "# Graphique en camembert\n",
    "axes[1].pie(attrition_counts, labels=['Non (Reste)', 'Oui (Part)'], autopct='%1.1f%%',\n",
    "            colors=colors, explode=(0, 0.1), shadow=True, startangle=90)\n",
    "axes[1].set_title('R√©partition de l\\'Attrition', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è CONSTAT : Classes d√©s√©quilibr√©es !\")\n",
    "print(\"   ‚Üí Nous devrons utiliser des techniques comme SMOTE ou class_weight pour g√©rer ce d√©s√©quilibre.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b42bd",
   "metadata": {},
   "source": [
    "### 2.3 Exploration des autres fichiers\n",
    "\n",
    "Examinons rapidement les autres sources de donn√©es avant de les fusionner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67f0343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration de manager_survey_data.csv\n",
    "print(\"=\" * 60)\n",
    "print(\"MANAGER_SURVEY_DATA - √âvaluations manag√©riales\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nAper√ßu :\")\n",
    "display(manager_survey.head())\n",
    "\n",
    "print(\"\\nDistribution des variables :\")\n",
    "print(\"\\nJobInvolvement (Implication au travail, √©chelle 1-4) :\")\n",
    "print(manager_survey['JobInvolvement'].value_counts().sort_index())\n",
    "print(\"\\nPerformanceRating (Note de performance, √©chelle 3-4) :\")\n",
    "print(manager_survey['PerformanceRating'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration de employee_survey_data.csv\n",
    "print(\"=\" * 60)\n",
    "print(\"EMPLOYEE_SURVEY_DATA - Enqu√™te satisfaction\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nAper√ßu :\")\n",
    "display(employee_survey.head())\n",
    "\n",
    "print(\"\\nValeurs manquantes (les 'NA' ont √©t√© convertis en NaN) :\")\n",
    "print(employee_survey.isnull().sum())\n",
    "\n",
    "print(\"\\nDistribution des variables (√©chelles 1-4) :\")\n",
    "for col in ['EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance']:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(employee_survey[col].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf0ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration de in_time.csv et out_time.csv\n",
    "print(\"=\" * 60)\n",
    "print(\"IN_TIME & OUT_TIME - Horaires d'arriv√©e et d√©part (2015)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nAper√ßu de in_time (5 premiers jours, 5 premiers employ√©s) :\")\n",
    "display(in_time.iloc[:5, :5])\n",
    "\n",
    "print(\"\\nAper√ßu de out_time (5 premiers jours, 5 premiers employ√©s) :\")\n",
    "display(out_time.iloc[:5, :5])\n",
    "\n",
    "print(\"\\nFormat des timestamps :\")\n",
    "print(f\"Exemple d'arriv√©e : {in_time.iloc[0, 0]}\")\n",
    "print(f\"Exemple de d√©part : {out_time.iloc[0, 0]}\")\n",
    "\n",
    "print(\"\\nValeurs manquantes (NA = jours non travaill√©s) :\")\n",
    "total_cells = in_time.shape[0] * in_time.shape[1]\n",
    "na_cells = in_time.isnull().sum().sum()\n",
    "print(f\"in_time : {na_cells} NA sur {total_cells} cellules ({100*na_cells/total_cells:.1f}%)\")\n",
    "na_cells_out = out_time.isnull().sum().sum()\n",
    "print(f\"out_time : {na_cells_out} NA sur {total_cells} cellules ({100*na_cells_out/total_cells:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cde3a5",
   "metadata": {},
   "source": [
    "## 3. Analyse des Valeurs Manquantes\n",
    "\n",
    "### Pourquoi cette √©tape est critique ?\n",
    "\n",
    "Les valeurs manquantes peuvent :\n",
    "1. **Biaiser les analyses** si elles ne sont pas al√©atoires\n",
    "2. **Emp√™cher l'entra√Ænement** de certains mod√®les ML\n",
    "3. **R√©v√©ler des patterns** (ex: employ√©s qui ne r√©pondent pas aux enqu√™tes)\n",
    "\n",
    "### Approche (inspir√©e du Workshop EDA)\n",
    "\n",
    "Nous utilisons la librairie `missingno` pour visualiser les valeurs manquantes de mani√®re intuitive :\n",
    "- **Matrix plot** : Vue d'ensemble des donn√©es manquantes\n",
    "- **Bar plot** : Comptage des valeurs non-nulles par colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cee6909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des valeurs manquantes avec missingno (comme dans le Workshop EDA)\n",
    "print(\"=\" * 60)\n",
    "print(\"VISUALISATION DES VALEURS MANQUANTES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyse pour general_data\n",
    "print(\"\\n1. GENERAL_DATA :\")\n",
    "print(f\"   Valeurs manquantes par colonne :\")\n",
    "missing_general = general_data.isnull().sum()\n",
    "missing_general = missing_general[missing_general > 0]\n",
    "if len(missing_general) > 0:\n",
    "    print(missing_general)\n",
    "else:\n",
    "    print(\"   Aucune valeur manquante !\")\n",
    "\n",
    "# Analyse pour employee_survey (celui qui contient des NA)\n",
    "print(\"\\n2. EMPLOYEE_SURVEY :\")\n",
    "print(f\"   Valeurs manquantes par colonne :\")\n",
    "print(employee_survey.isnull().sum())\n",
    "\n",
    "# Visualisation avec missingno\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Matrix plot pour employee_survey\n",
    "plt.subplot(1, 2, 1)\n",
    "msno.matrix(employee_survey, ax=axes[0], sparkline=False)\n",
    "axes[0].set_title('Valeurs Manquantes - Employee Survey', fontsize=12)\n",
    "\n",
    "# Bar plot pour general_data\n",
    "plt.subplot(1, 2, 2)\n",
    "msno.bar(general_data, ax=axes[1])\n",
    "axes[1].set_title('Valeurs Non-Nulles - General Data', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97da5497",
   "metadata": {},
   "source": [
    "## 4. Fusion des Datasets\n",
    "\n",
    "### Types de jointures SQL/Pandas\n",
    "\n",
    "| Type | Description | Usage |\n",
    "|------|-------------|-------|\n",
    "| `inner` | Garde uniquement les lignes pr√©sentes dans les deux tables | Intersection stricte |\n",
    "| `left` | Garde toutes les lignes de la table de gauche | Pr√©serve le dataset principal |\n",
    "| `right` | Garde toutes les lignes de la table de droite | Rarement utilis√© |\n",
    "| `outer` | Garde toutes les lignes des deux tables | Union compl√®te |\n",
    "\n",
    "### Notre strat√©gie\n",
    "\n",
    "Nous utilisons **`left join`** avec `general_data` comme table principale car :\n",
    "1. C'est le fichier contenant la variable cible `Attrition`\n",
    "2. Tous les EmployeeID doivent √™tre pr√©sents dans ce fichier\n",
    "3. Nous ne voulons pas perdre de donn√©es du fichier principal\n",
    "\n",
    "```\n",
    "general_data (4411 lignes)\n",
    "    ‚Üê LEFT JOIN ‚Üí manager_survey (4411 lignes)\n",
    "    ‚Üê LEFT JOIN ‚Üí employee_survey (4411 lignes)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ec72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUSION DES DATASETS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FUSION DES DATASETS SUR EmployeeID\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# V√©rification de la cl√© de jointure\n",
    "print(\"\\nV√©rification des EmployeeID :\")\n",
    "print(f\"  general_data : {general_data['EmployeeID'].nunique()} IDs uniques\")\n",
    "print(f\"  manager_survey : {manager_survey['EmployeeID'].nunique()} IDs uniques\")\n",
    "print(f\"  employee_survey : {employee_survey['EmployeeID'].nunique()} IDs uniques\")\n",
    "\n",
    "# V√©rification des doublons sur la cl√©\n",
    "print(\"\\nDoublons sur EmployeeID :\")\n",
    "print(f\"  general_data : {general_data['EmployeeID'].duplicated().sum()} doublons\")\n",
    "print(f\"  manager_survey : {manager_survey['EmployeeID'].duplicated().sum()} doublons\")\n",
    "print(f\"  employee_survey : {employee_survey['EmployeeID'].duplicated().sum()} doublons\")\n",
    "\n",
    "# Fusion √©tape par √©tape\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"√âtape 1 : general_data + manager_survey\")\n",
    "df = pd.merge(general_data, manager_survey, on='EmployeeID', how='left')\n",
    "print(f\"R√©sultat : {df.shape[0]} lignes √ó {df.shape[1]} colonnes\")\n",
    "\n",
    "print(\"\\n√âtape 2 : + employee_survey\")\n",
    "df = pd.merge(df, employee_survey, on='EmployeeID', how='left')\n",
    "print(f\"R√©sultat : {df.shape[0]} lignes √ó {df.shape[1]} colonnes\")\n",
    "\n",
    "# V√©rification finale\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"V√âRIFICATION POST-FUSION :\")\n",
    "print(f\"  Lignes conserv√©es : {df.shape[0]} (attendu : {general_data.shape[0]})\")\n",
    "print(f\"  Colonnes totales : {df.shape[1]}\")\n",
    "print(f\"  Valeurs manquantes totales : {df.isnull().sum().sum()}\")\n",
    "\n",
    "if df.shape[0] == general_data.shape[0]:\n",
    "    print(\"\\n‚úì Fusion r√©ussie ! Aucune ligne perdue.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è ATTENTION : Nombre de lignes diff√©rent !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00e279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aper√ßu du dataset fusionn√©\n",
    "print(\"=\" * 60)\n",
    "print(\"APER√áU DU DATASET FUSIONN√â\")\n",
    "print(\"=\" * 60)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575486a8",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering : M√©triques d'Horaires\n",
    "\n",
    "### Pourquoi les horaires sont importants ?\n",
    "\n",
    "Les donn√©es d'horaires (`in_time` et `out_time`) peuvent r√©v√©ler :\n",
    "- **Overtime** : Heures suppl√©mentaires chroniques ‚Üí stress, burnout\n",
    "- **R√©gularit√©** : Variabilit√© des horaires ‚Üí missions changeantes, instabilit√©\n",
    "- **Ponctualit√©** : Arriv√©es tardives, d√©parts anticip√©s ‚Üí d√©sengagement potentiel\n",
    "- **Pr√©sent√©isme** : Nombre de jours travaill√©s ‚Üí absent√©isme\n",
    "\n",
    "### M√©triques calcul√©es\n",
    "\n",
    "| M√©trique | Description | Hypoth√®se |\n",
    "|----------|-------------|-----------|\n",
    "| `avg_hours` | Moyenne des heures travaill√©es par jour | Charge de travail |\n",
    "| `std_hours` | √âcart-type des heures | R√©gularit√©/Variabilit√© |\n",
    "| `work_days` | Nombre de jours travaill√©s (non-NA) | Pr√©sent√©isme |\n",
    "| `late_arrivals` | Jours avec arriv√©e apr√®s 9h30 | Ponctualit√© |\n",
    "| `early_departures` | Jours avec d√©part avant 17h00 | Engagement |\n",
    "| `overtime_days` | Jours avec plus de 9h de travail | Surcharge |\n",
    "\n",
    "### Justification des seuils\n",
    "\n",
    "- **9h30** pour arriv√©e tardive : Horaire de d√©but standard dans de nombreuses entreprises est 9h00, avec 30min de tol√©rance\n",
    "- **17h00** pour d√©part anticip√© : Journ√©e de 8h standard (9h-17h avec pause)\n",
    "- **9h** pour overtime : Au-del√† de 8h de travail effectif = heures suppl√©mentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a426d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE ENGINEERING : CALCUL DES M√âTRIQUES D'HORAIRES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CALCUL DES M√âTRIQUES D'HORAIRES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Conversion des timestamps en datetime\n",
    "# Les colonnes sont des dates (ex: \"2015-01-02\")\n",
    "# Les valeurs sont des timestamps (ex: \"2015-01-02 09:43:45\")\n",
    "\n",
    "print(\"\\n√âtape 1 : Conversion des timestamps...\")\n",
    "\n",
    "# Fonction pour calculer les heures travaill√©es par cellule\n",
    "def calculate_hours_worked(in_ts, out_ts):\n",
    "    \"\"\"\n",
    "    Calcule le nombre d'heures travaill√©es entre deux timestamps.\n",
    "    Retourne NaN si l'une des valeurs est manquante.\n",
    "    \"\"\"\n",
    "    if pd.isna(in_ts) or pd.isna(out_ts):\n",
    "        return np.nan\n",
    "    try:\n",
    "        in_dt = pd.to_datetime(in_ts)\n",
    "        out_dt = pd.to_datetime(out_ts)\n",
    "        hours = (out_dt - in_dt).total_seconds() / 3600\n",
    "        # V√©rification de coh√©rence (entre 0 et 24 heures)\n",
    "        if 0 < hours < 24:\n",
    "            return hours\n",
    "        else:\n",
    "            return np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Calcul des heures travaill√©es pour chaque jour et chaque employ√©\n",
    "print(\"Calcul des heures travaill√©es pour chaque jour...\")\n",
    "hours_worked = pd.DataFrame(index=in_time.index, columns=in_time.columns)\n",
    "\n",
    "for col in in_time.columns:\n",
    "    hours_worked[col] = [calculate_hours_worked(in_time.loc[emp, col], out_time.loc[emp, col]) \n",
    "                          for emp in in_time.index]\n",
    "    \n",
    "# Conversion en float\n",
    "hours_worked = hours_worked.astype(float)\n",
    "print(f\"‚úì Matrice des heures calcul√©e : {hours_worked.shape}\")\n",
    "\n",
    "# Aper√ßu\n",
    "print(\"\\nAper√ßu des heures travaill√©es (5 premiers jours, 5 premiers employ√©s) :\")\n",
    "display(hours_worked.iloc[:5, :5].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac76b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# AGR√âGATION DES M√âTRIQUES PAR EMPLOY√â\n",
    "# =============================================================================\n",
    "\n",
    "print(\"√âtape 2 : Agr√©gation des m√©triques par employ√©...\")\n",
    "\n",
    "# Cr√©ation du DataFrame des m√©triques agr√©g√©es\n",
    "time_metrics = pd.DataFrame(index=in_time.index)\n",
    "\n",
    "# 1. Moyenne des heures travaill√©es\n",
    "time_metrics['avg_hours'] = hours_worked.mean(axis=1)\n",
    "\n",
    "# 2. √âcart-type des heures (r√©gularit√©)\n",
    "time_metrics['std_hours'] = hours_worked.std(axis=1)\n",
    "\n",
    "# 3. Nombre de jours travaill√©s (non-NA)\n",
    "time_metrics['work_days'] = hours_worked.notna().sum(axis=1)\n",
    "\n",
    "# 4. Nombre de jours avec overtime (> 9 heures)\n",
    "time_metrics['overtime_days'] = (hours_worked > 9).sum(axis=1)\n",
    "\n",
    "# 5. Arriv√©es tardives (apr√®s 9h30) et d√©parts anticip√©s (avant 17h00)\n",
    "# Nous devons recalculer en utilisant les heures d'arriv√©e/d√©part\n",
    "late_arrivals = pd.DataFrame(index=in_time.index, columns=in_time.columns)\n",
    "early_departures = pd.DataFrame(index=in_time.index, columns=in_time.columns)\n",
    "\n",
    "for col in in_time.columns:\n",
    "    for emp in in_time.index:\n",
    "        in_ts = in_time.loc[emp, col]\n",
    "        out_ts = out_time.loc[emp, col]\n",
    "        \n",
    "        if pd.notna(in_ts):\n",
    "            try:\n",
    "                in_hour = pd.to_datetime(in_ts).hour + pd.to_datetime(in_ts).minute / 60\n",
    "                late_arrivals.loc[emp, col] = 1 if in_hour > 9.5 else 0\n",
    "            except:\n",
    "                late_arrivals.loc[emp, col] = np.nan\n",
    "        else:\n",
    "            late_arrivals.loc[emp, col] = np.nan\n",
    "            \n",
    "        if pd.notna(out_ts):\n",
    "            try:\n",
    "                out_hour = pd.to_datetime(out_ts).hour + pd.to_datetime(out_ts).minute / 60\n",
    "                early_departures.loc[emp, col] = 1 if out_hour < 17 else 0\n",
    "            except:\n",
    "                early_departures.loc[emp, col] = np.nan\n",
    "        else:\n",
    "            early_departures.loc[emp, col] = np.nan\n",
    "\n",
    "time_metrics['late_arrivals'] = late_arrivals.sum(axis=1)\n",
    "time_metrics['early_departures'] = early_departures.sum(axis=1)\n",
    "\n",
    "# Renommer l'index pour la jointure\n",
    "time_metrics.index.name = 'EmployeeID'\n",
    "time_metrics = time_metrics.reset_index()\n",
    "\n",
    "print(\"‚úì M√©triques d'horaires calcul√©es !\")\n",
    "print(f\"\\nDimensions : {time_metrics.shape[0]} employ√©s √ó {time_metrics.shape[1]} m√©triques\")\n",
    "\n",
    "# Aper√ßu\n",
    "display(time_metrics.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f74e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives des m√©triques d'horaires\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTIQUES DES M√âTRIQUES D'HORAIRES\")\n",
    "print(\"=\" * 60)\n",
    "display(time_metrics.describe())\n",
    "\n",
    "# Visualisation des distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "metrics = ['avg_hours', 'std_hours', 'work_days', 'overtime_days', 'late_arrivals', 'early_departures']\n",
    "titles = ['Heures moyennes/jour', '√âcart-type heures', 'Jours travaill√©s', \n",
    "          'Jours avec overtime', 'Arriv√©es tardives', 'D√©parts anticip√©s']\n",
    "\n",
    "for ax, metric, title in zip(axes.flatten(), metrics, titles):\n",
    "    sns.histplot(time_metrics[metric], ax=ax, kde=True, color='steelblue')\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('')\n",
    "    \n",
    "plt.suptitle('Distribution des M√©triques d\\'Horaires', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e35d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUSION DES M√âTRIQUES D'HORAIRES AU DATASET PRINCIPAL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FUSION DES M√âTRIQUES D'HORAIRES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nDataset avant fusion : {df.shape[0]} lignes √ó {df.shape[1]} colonnes\")\n",
    "\n",
    "# Fusion avec left join\n",
    "df = pd.merge(df, time_metrics, on='EmployeeID', how='left')\n",
    "\n",
    "print(f\"Dataset apr√®s fusion : {df.shape[0]} lignes √ó {df.shape[1]} colonnes\")\n",
    "print(f\"\\nNouvelles colonnes ajout√©es : {list(time_metrics.columns[1:])}\")\n",
    "\n",
    "# V√©rification\n",
    "print(f\"\\nValeurs manquantes dans les nouvelles colonnes :\")\n",
    "print(df[['avg_hours', 'std_hours', 'work_days', 'overtime_days', 'late_arrivals', 'early_departures']].isnull().sum())\n",
    "\n",
    "print(\"\\n‚úì Dataset complet cr√©√© avec succ√®s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dffa08",
   "metadata": {},
   "source": [
    "## 6. Nettoyage et Pr√©paration des Donn√©es\n",
    "\n",
    "### √âtapes de pr√©paration\n",
    "\n",
    "1. **Suppression des colonnes inutiles** : Colonnes avec valeurs constantes (n'apportent pas d'information)\n",
    "2. **Traitement des valeurs manquantes** : Imputation par m√©diane (num√©riques) ou mode (cat√©gorielles)\n",
    "3. **Encodage de la variable cible** : Attrition Yes‚Üí1, No‚Üí0\n",
    "4. **Encodage des variables cat√©gorielles** : OneHotEncoding pour les variables nominales\n",
    "5. **Normalisation** : StandardScaler pour les mod√®les sensibles √† l'√©chelle\n",
    "\n",
    "### Pipeline sklearn (inspir√© du Workshop R√©gression)\n",
    "\n",
    "La cr√©ation d'une pipeline permet de :\n",
    "- **Automatiser** le pr√©traitement\n",
    "- **√âviter le data leakage** (fit uniquement sur le train set)\n",
    "- **Reproduire** facilement les transformations sur de nouvelles donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9372b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NETTOYAGE : SUPPRESSION DES COLONNES INUTILES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"IDENTIFICATION DES COLONNES √Ä SUPPRIMER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identification des colonnes avec valeurs constantes\n",
    "cols_to_drop = []\n",
    "\n",
    "for col in df.columns:\n",
    "    unique_values = df[col].nunique()\n",
    "    if unique_values == 1:\n",
    "        print(f\"  ‚ö†Ô∏è {col} : {unique_values} valeur unique ‚Üí '{df[col].iloc[0]}' ‚Üí √Ä SUPPRIMER\")\n",
    "        cols_to_drop.append(col)\n",
    "\n",
    "# Colonnes sp√©cifiques √† supprimer (identifi√©es lors de l'exploration)\n",
    "additional_drops = ['EmployeeID']  # L'ID n'est pas une feature pr√©dictive\n",
    "cols_to_drop.extend(additional_drops)\n",
    "\n",
    "print(f\"\\n  ‚ö†Ô∏è EmployeeID : Identifiant, non pr√©dictif ‚Üí √Ä SUPPRIMER\")\n",
    "\n",
    "print(f\"\\nColonnes √† supprimer : {cols_to_drop}\")\n",
    "\n",
    "# Suppression\n",
    "df_clean = df.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"\\nDataset apr√®s nettoyage :\")\n",
    "print(f\"  Avant : {df.shape[0]} lignes √ó {df.shape[1]} colonnes\")\n",
    "print(f\"  Apr√®s : {df_clean.shape[0]} lignes √ó {df_clean.shape[1]} colonnes\")\n",
    "print(f\"  Colonnes supprim√©es : {len(cols_to_drop)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50234bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAITEMENT DES VALEURS MANQUANTES (comme dans Workshop EDA)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAITEMENT DES VALEURS MANQUANTES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identification des colonnes avec valeurs manquantes\n",
    "missing_cols = df_clean.isnull().sum()\n",
    "missing_cols = missing_cols[missing_cols > 0]\n",
    "\n",
    "print(\"\\nColonnes avec valeurs manquantes :\")\n",
    "for col, count in missing_cols.items():\n",
    "    pct = 100 * count / len(df_clean)\n",
    "    dtype = df_clean[col].dtype\n",
    "    print(f\"  {col}: {count} manquantes ({pct:.2f}%) - Type: {dtype}\")\n",
    "\n",
    "# S√©paration des colonnes num√©riques et cat√©gorielles\n",
    "num_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Suppression de la variable cible des colonnes √† imputer\n",
    "if 'Attrition' in cat_cols:\n",
    "    cat_cols.remove('Attrition')\n",
    "\n",
    "print(f\"\\nColonnes num√©riques : {len(num_cols)}\")\n",
    "print(f\"Colonnes cat√©gorielles (hors cible) : {len(cat_cols)}\")\n",
    "\n",
    "# Imputation\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"Imputation des valeurs manquantes :\")\n",
    "\n",
    "# Num√©riques : imputation par m√©diane (robuste aux outliers)\n",
    "for col in num_cols:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        median_val = df_clean[col].median()\n",
    "        df_clean[col].fillna(median_val, inplace=True)\n",
    "        print(f\"  ‚úì {col} : imputation par m√©diane ({median_val:.2f})\")\n",
    "\n",
    "# Cat√©gorielles : imputation par mode\n",
    "for col in cat_cols:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        mode_val = df_clean[col].mode()[0]\n",
    "        df_clean[col].fillna(mode_val, inplace=True)\n",
    "        print(f\"  ‚úì {col} : imputation par mode ('{mode_val}')\")\n",
    "\n",
    "# V√©rification\n",
    "remaining_na = df_clean.isnull().sum().sum()\n",
    "print(f\"\\nValeurs manquantes restantes : {remaining_na}\")\n",
    "\n",
    "if remaining_na == 0:\n",
    "    print(\"‚úì Toutes les valeurs manquantes ont √©t√© trait√©es !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513cbd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENCODAGE DE LA VARIABLE CIBLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ENCODAGE DE LA VARIABLE CIBLE : ATTRITION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Encodage binaire : Yes ‚Üí 1, No ‚Üí 0\n",
    "print(\"\\nAvant encodage :\")\n",
    "print(df_clean['Attrition'].value_counts())\n",
    "\n",
    "df_clean['Attrition'] = df_clean['Attrition'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "print(\"\\nApr√®s encodage :\")\n",
    "print(df_clean['Attrition'].value_counts())\n",
    "\n",
    "print(\"\\n‚úì Variable cible encod√©e : Yes=1 (d√©part), No=0 (reste)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b01173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CR√âATION DE LA PIPELINE DE PR√âPARATION (style Workshop R√©gression)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURATION DE LA PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# S√©paration X (features) et y (cible)\n",
    "X = df_clean.drop('Attrition', axis=1)\n",
    "y = df_clean['Attrition']\n",
    "\n",
    "print(f\"Features (X) : {X.shape[0]} lignes √ó {X.shape[1]} colonnes\")\n",
    "print(f\"Cible (y) : {y.shape[0]} valeurs\")\n",
    "\n",
    "# Identification des colonnes num√©riques et cat√©gorielles\n",
    "num_attribs = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_attribs = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nColonnes num√©riques ({len(num_attribs)}) :\")\n",
    "print(f\"  {num_attribs}\")\n",
    "print(f\"\\nColonnes cat√©gorielles ({len(cat_attribs)}) :\")\n",
    "print(f\"  {cat_attribs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb3b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline num√©rique (imputation + normalisation)\n",
    "# Inspir√©e du Workshop R√©gression - sklearn Pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),  # Imputation par m√©diane\n",
    "    ('std_scaler', StandardScaler()),               # Normalisation (moyenne=0, std=1)\n",
    "])\n",
    "\n",
    "# Pipeline complet avec ColumnTransformer (traitement diff√©renci√© num/cat)\n",
    "# Comme dans le Workshop R√©gression\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_attribs),\n",
    "])\n",
    "\n",
    "print(\"Pipeline configur√©e :\")\n",
    "print(\"  1. Num√©riques : Imputation m√©diane ‚Üí StandardScaler\")\n",
    "print(\"  2. Cat√©gorielles : OneHotEncoder\")\n",
    "\n",
    "# Application de la pipeline sur X\n",
    "X_prepared = full_pipeline.fit_transform(X)\n",
    "\n",
    "# R√©cup√©ration des noms de colonnes apr√®s transformation\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.get_feature_names_out(cat_attribs))\n",
    "columns = num_attribs + cat_one_hot_attribs\n",
    "\n",
    "# Conversion en DataFrame pour lisibilit√©\n",
    "X_prepared_df = pd.DataFrame(X_prepared, columns=columns, index=X.index)\n",
    "\n",
    "print(f\"\\nDataset transform√© : {X_prepared_df.shape[0]} lignes √ó {X_prepared_df.shape[1]} colonnes\")\n",
    "print(f\"  Colonnes num√©riques : {len(num_attribs)}\")\n",
    "print(f\"  Colonnes one-hot encod√©es : {len(cat_one_hot_attribs)}\")\n",
    "\n",
    "display(X_prepared_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ba42c2",
   "metadata": {},
   "source": [
    "### 6.1 Split Train/Test Stratifi√©\n",
    "\n",
    "#### Pourquoi la stratification est cruciale ?\n",
    "\n",
    "Avec des classes d√©s√©quilibr√©es (85% No / 15% Yes), un split al√©atoire pourrait :\n",
    "- Cr√©er un test set avec 20% de Yes et un train set avec 10% de Yes\n",
    "- Biaiser l'√©valuation du mod√®le\n",
    "\n",
    "La **stratification** garantit que les proportions de classes sont pr√©serv√©es dans train et test.\n",
    "\n",
    "#### M√©thode utilis√©e\n",
    "\n",
    "Comme dans le Workshop R√©gression, nous utilisons `StratifiedShuffleSplit` :\n",
    "```python\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "- `n_splits=1` : Un seul split (pas de cross-validation √† ce stade)\n",
    "- `test_size=0.2` : 20% pour le test, 80% pour l'entra√Ænement\n",
    "- `random_state=42` : Reproductibilit√© des r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093b9d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPLIT TRAIN/TEST STRATIFI√â (comme dans Workshop R√©gression)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SPLIT TRAIN/TEST STRATIFI√â\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Utilisation de StratifiedShuffleSplit (comme dans le Workshop)\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "for train_index, test_index in split.split(X_prepared_df, y):\n",
    "    X_train = X_prepared_df.iloc[train_index]\n",
    "    X_test = X_prepared_df.iloc[test_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "\n",
    "print(f\"\\nDimensions :\")\n",
    "print(f\"  X_train : {X_train.shape[0]} lignes √ó {X_train.shape[1]} colonnes\")\n",
    "print(f\"  X_test  : {X_test.shape[0]} lignes √ó {X_test.shape[1]} colonnes\")\n",
    "print(f\"  y_train : {y_train.shape[0]} valeurs\")\n",
    "print(f\"  y_test  : {y_test.shape[0]} valeurs\")\n",
    "\n",
    "# V√©rification de la stratification\n",
    "print(\"\\nV√©rification de la stratification :\")\n",
    "print(f\"  Distribution originale : {y.value_counts(normalize=True).round(4).to_dict()}\")\n",
    "print(f\"  Distribution train     : {y_train.value_counts(normalize=True).round(4).to_dict()}\")\n",
    "print(f\"  Distribution test      : {y_test.value_counts(normalize=True).round(4).to_dict()}\")\n",
    "\n",
    "print(\"\\n‚úì Proportions pr√©serv√©es dans train et test !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03072e4a",
   "metadata": {},
   "source": [
    "## 7. Analyse Exploratoire Approfondie (EDA)\n",
    "\n",
    "### Objectifs de l'EDA\n",
    "\n",
    "1. **Comprendre les relations** entre les variables et l'attrition\n",
    "2. **Identifier les facteurs cl√©s** qui diff√©rencient les employ√©s qui partent\n",
    "3. **D√©tecter les patterns** et tendances dans les donn√©es\n",
    "4. **Guider la mod√©lisation** en identifiant les features les plus prometteuses\n",
    "\n",
    "### Types d'analyses\n",
    "\n",
    "| Type | Description | Visualisations |\n",
    "|------|-------------|----------------|\n",
    "| Univari√©e | Distribution d'une seule variable | Histogrammes, countplots |\n",
    "| Bivari√©e | Relation entre 2 variables | Boxplots, scatter plots |\n",
    "| Multivari√©e | Relations entre plusieurs variables | Heatmaps, pairplots |\n",
    "\n",
    "> **Note** : Nous utilisons les donn√©es **avant transformation** (`df_clean`) pour l'EDA car les valeurs originales sont plus interpr√©tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f69cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HEATMAP DE CORR√âLATION (comme dans Workshop EDA)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HEATMAP DE CORR√âLATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# S√©lection des colonnes num√©riques pour la corr√©lation\n",
    "# On utilise df_clean avec Attrition encod√©\n",
    "numeric_df = df_clean.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calcul de la matrice de corr√©lation\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Corr√©lations avec la variable cible\n",
    "attrition_corr = correlation_matrix['Attrition'].sort_values(ascending=False)\n",
    "print(\"\\nTop 10 corr√©lations avec Attrition :\")\n",
    "print(attrition_corr.head(11))  # 11 car Attrition avec elle-m√™me = 1\n",
    "\n",
    "print(\"\\nTop 10 corr√©lations n√©gatives avec Attrition :\")\n",
    "print(attrition_corr.tail(10))\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(16, 12))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "            cmap='RdBu_r', center=0, linewidths=0.5,\n",
    "            annot_kws={\"size\": 8})\n",
    "plt.title('Matrice de Corr√©lation\\n(Triangle inf√©rieur)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° INTERPR√âTATION :\")\n",
    "print(\"  - Valeurs proches de +1 : corr√©lation positive forte\")\n",
    "print(\"  - Valeurs proches de -1 : corr√©lation n√©gative forte\")\n",
    "print(\"  - Valeurs proches de 0 : pas de corr√©lation lin√©aire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95966239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CORR√âLATIONS SP√âCIFIQUES AVEC ATTRITION\n",
    "# =============================================================================\n",
    "\n",
    "# Visualisation des corr√©lations avec Attrition (barplot horizontal)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Exclure Attrition de la liste\n",
    "attrition_corr_plot = attrition_corr.drop('Attrition')\n",
    "\n",
    "# Couleurs selon le signe\n",
    "colors = ['#e74c3c' if x > 0 else '#2ecc71' for x in attrition_corr_plot]\n",
    "\n",
    "# Barplot\n",
    "attrition_corr_plot.plot(kind='barh', color=colors, edgecolor='black')\n",
    "plt.axvline(x=0, color='black', linewidth=0.8)\n",
    "plt.xlabel('Coefficient de Corr√©lation')\n",
    "plt.ylabel('Variables')\n",
    "plt.title('Corr√©lation des Variables avec l\\'Attrition', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Lignes de r√©f√©rence pour corr√©lations faibles/moyennes/fortes\n",
    "plt.axvline(x=0.1, color='gray', linestyle='--', alpha=0.5, label='Faible (¬±0.1)')\n",
    "plt.axvline(x=-0.1, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=0.3, color='orange', linestyle='--', alpha=0.5, label='Moyenne (¬±0.3)')\n",
    "plt.axvline(x=-0.3, color='orange', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° OBSERVATIONS CL√âS :\")\n",
    "print(\"  - Variables avec corr√©lation POSITIVE : favorisent l'attrition\")\n",
    "print(\"  - Variables avec corr√©lation N√âGATIVE : r√©duisent l'attrition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73080736",
   "metadata": {},
   "source": [
    "### 7.1 Analyse Bivari√©e : Variables Num√©riques vs Attrition\n",
    "\n",
    "Les **boxplots** permettent de comparer les distributions d'une variable num√©rique entre les deux groupes (Attrition = 0 vs 1).\n",
    "\n",
    "**Comment lire un boxplot ?**\n",
    "- La bo√Æte repr√©sente les quartiles Q1-Q3 (50% des donn√©es centrales)\n",
    "- La ligne au milieu = m√©diane\n",
    "- Les moustaches s'√©tendent jusqu'√† 1.5 √ó IQR\n",
    "- Les points au-del√† sont des outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af92660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BOXPLOTS : Variables num√©riques vs Attrition (style Workshop EDA)\n",
    "# =============================================================================\n",
    "\n",
    "# S√©lection des variables num√©riques les plus pertinentes\n",
    "key_numeric_vars = ['Age', 'MonthlyIncome', 'YearsAtCompany', 'TotalWorkingYears',\n",
    "                    'DistanceFromHome', 'NumCompaniesWorked', 'YearsSinceLastPromotion',\n",
    "                    'avg_hours', 'overtime_days', 'late_arrivals']\n",
    "\n",
    "# V√©rifier que toutes les colonnes existent\n",
    "key_numeric_vars = [col for col in key_numeric_vars if col in df_clean.columns]\n",
    "\n",
    "n_vars = len(key_numeric_vars)\n",
    "n_cols = 3\n",
    "n_rows = (n_vars + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, var in enumerate(key_numeric_vars):\n",
    "    sns.boxplot(x='Attrition', y=var, data=df_clean, ax=axes[i], \n",
    "                palette={0: '#2ecc71', 1: '#e74c3c'})\n",
    "    axes[i].set_title(f'{var} vs Attrition', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel('Attrition (0=Non, 1=Oui)')\n",
    "    axes[i].set_ylabel(var)\n",
    "\n",
    "# Masquer les axes vides\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle('Distribution des Variables Num√©riques par Statut d\\'Attrition', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c761b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TAUX D'ATTRITION PAR CAT√âGORIE\n",
    "# =============================================================================\n",
    "\n",
    "# Pour les variables cat√©gorielles, analysons le taux d'attrition par cat√©gorie\n",
    "cat_vars = ['Department', 'JobRole', 'MaritalStatus', 'BusinessTravel', \n",
    "            'Gender', 'EducationField']\n",
    "\n",
    "# Filtrer les variables qui existent\n",
    "cat_vars = [col for col in cat_vars if col in df_clean.columns]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, var in enumerate(cat_vars):\n",
    "    # Calculer le taux d'attrition par cat√©gorie\n",
    "    attrition_rate = df_clean.groupby(var)['Attrition'].mean().sort_values(ascending=False)\n",
    "    \n",
    "    # Barplot\n",
    "    colors = plt.cm.RdYlGn_r(attrition_rate / attrition_rate.max())\n",
    "    attrition_rate.plot(kind='bar', ax=axes[i], color=colors, edgecolor='black')\n",
    "    \n",
    "    # Ligne horizontale pour le taux global\n",
    "    global_rate = df_clean['Attrition'].mean()\n",
    "    axes[i].axhline(y=global_rate, color='red', linestyle='--', linewidth=2, \n",
    "                    label=f'Taux global ({global_rate:.1%})')\n",
    "    \n",
    "    axes[i].set_title(f'Taux d\\'Attrition par {var}', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('Taux d\\'Attrition')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    axes[i].legend()\n",
    "    \n",
    "    # Formater en pourcentage\n",
    "    axes[i].yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "\n",
    "plt.suptitle('Taux d\\'Attrition par Variable Cat√©gorielle', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° INTERPR√âTATION :\")\n",
    "print(\"  - Les barres AU-DESSUS de la ligne rouge = cat√©gories √† risque\")\n",
    "print(\"  - Les barres EN-DESSOUS de la ligne rouge = cat√©gories moins √† risque\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb9c18",
   "metadata": {},
   "source": [
    "## 8. Mod√©lisation\n",
    "\n",
    "### Introduction √† la Classification Binaire\n",
    "\n",
    "Notre objectif est de pr√©dire une variable binaire `Attrition` (0 ou 1). C'est un probl√®me de **classification binaire**.\n",
    "\n",
    "### M√©triques d'√©valuation\n",
    "\n",
    "| M√©trique | D√©finition | Interpr√©tation pour l'attrition |\n",
    "|----------|------------|--------------------------------|\n",
    "| **Accuracy** | $\\frac{TP + TN}{Total}$ | % de pr√©dictions correctes (biais√© si classes d√©s√©quilibr√©es) |\n",
    "| **Precision** | $\\frac{TP}{TP + FP}$ | Parmi les pr√©dictions \"d√©part\", combien sont correctes ? |\n",
    "| **Recall** | $\\frac{TP}{TP + FN}$ | Parmi les vrais d√©parts, combien sont d√©tect√©s ? |\n",
    "| **F1-Score** | $2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$ | Moyenne harmonique Precision/Recall |\n",
    "| **ROC-AUC** | Aire sous la courbe ROC | Capacit√© globale de discrimination |\n",
    "\n",
    "### Matrice de confusion\n",
    "\n",
    "```\n",
    "                  Pr√©dit Non    Pr√©dit Oui\n",
    "R√©el Non (reste)      TN            FP      ‚Üê Fausses alertes\n",
    "R√©el Oui (part)       FN            TP      ‚Üê Vrais positifs\n",
    "                       ‚Üë\n",
    "                  Manqu√©s (co√ªteux!)\n",
    "```\n",
    "\n",
    "**Dans notre contexte RH :**\n",
    "- **FN (Faux N√©gatifs)** = Employ√©s qui partent mais qu'on n'a pas d√©tect√©s ‚Üí **CO√õT √âLEV√â**\n",
    "- **FP (Faux Positifs)** = Employ√©s qu'on pense √† risque mais qui restent ‚Üí Co√ªt mod√©r√© (actions pr√©ventives inutiles)\n",
    "\n",
    "‚Üí On privil√©gie le **Recall** (minimiser les FN) ou le **F1-Score** (√©quilibre)\n",
    "\n",
    "### Gestion du d√©s√©quilibre de classes\n",
    "\n",
    "Avec ~85% No / ~15% Yes, un mod√®le na√Øf qui pr√©dit toujours \"No\" aurait 85% d'accuracy mais 0% de Recall !\n",
    "\n",
    "**Solutions :**\n",
    "1. **SMOTE** (Synthetic Minority Over-sampling Technique) : Cr√©e des exemples synth√©tiques de la classe minoritaire\n",
    "2. **class_weight='balanced'** : P√©nalise davantage les erreurs sur la classe minoritaire\n",
    "3. **Undersampling** : R√©duit la classe majoritaire (perte d'information)\n",
    "\n",
    "Nous utiliserons **SMOTE** car c'est la technique la plus efficace sans perte de donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55004da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# APPLICATION DE SMOTE POUR √âQUILIBRER LES CLASSES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GESTION DU D√âS√âQUILIBRE : SMOTE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nAvant SMOTE :\")\n",
    "print(f\"  Classe 0 (reste) : {sum(y_train == 0)} √©chantillons\")\n",
    "print(f\"  Classe 1 (part)  : {sum(y_train == 1)} √©chantillons\")\n",
    "print(f\"  Ratio : {sum(y_train == 0) / sum(y_train == 1):.2f}\")\n",
    "\n",
    "# Application de SMOTE uniquement sur le train set\n",
    "# IMPORTANT : Ne jamais appliquer SMOTE sur le test set !\n",
    "smote = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nApr√®s SMOTE :\")\n",
    "print(f\"  Classe 0 (reste) : {sum(y_train_resampled == 0)} √©chantillons\")\n",
    "print(f\"  Classe 1 (part)  : {sum(y_train_resampled == 1)} √©chantillons\")\n",
    "print(f\"  Ratio : {sum(y_train_resampled == 0) / sum(y_train_resampled == 1):.2f}\")\n",
    "\n",
    "print(\"\\n‚úì Classes √©quilibr√©es ! Le mod√®le peut maintenant apprendre les deux classes √©quitablement.\")\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Avant SMOTE\n",
    "pd.Series(y_train).value_counts().plot(kind='bar', ax=axes[0], \n",
    "                                         color=['#2ecc71', '#e74c3c'], edgecolor='black')\n",
    "axes[0].set_title('Distribution AVANT SMOTE', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Attrition')\n",
    "axes[0].set_ylabel('Nombre')\n",
    "axes[0].set_xticklabels(['Non (0)', 'Oui (1)'], rotation=0)\n",
    "\n",
    "# Apr√®s SMOTE\n",
    "pd.Series(y_train_resampled).value_counts().plot(kind='bar', ax=axes[1], \n",
    "                                                   color=['#2ecc71', '#e74c3c'], edgecolor='black')\n",
    "axes[1].set_title('Distribution APR√àS SMOTE', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Attrition')\n",
    "axes[1].set_ylabel('Nombre')\n",
    "axes[1].set_xticklabels(['Non (0)', 'Oui (1)'], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b29ac",
   "metadata": {},
   "source": [
    "### 8.1 Comparaison Multi-Mod√®les (style Workshop R√©gression)\n",
    "\n",
    "Nous allons tester plusieurs algorithmes de classification et comparer leurs performances :\n",
    "\n",
    "| Mod√®le | Type | Avantages | Inconv√©nients |\n",
    "|--------|------|-----------|---------------|\n",
    "| **Logistic Regression** | Lin√©aire | Interpr√©table, rapide | Suppose lin√©arit√© |\n",
    "| **Decision Tree** | Arbre | Interpr√©table, non-lin√©aire | Overfitting facile |\n",
    "| **Random Forest** | Ensemble | Robuste, feature importance | Moins interpr√©table |\n",
    "| **XGBoost** | Boosting | Tr√®s performant | Complexe √† tuner |\n",
    "| **Gradient Boosting** | Boosting | Bon compromis | Plus lent |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761532ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPARAISON MULTI-MOD√àLES (inspir√© du Workshop R√©gression \"Bonus\")\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ENTRA√éNEMENT ET COMPARAISON DES MOD√àLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# D√©finition des mod√®les (comme dans le Workshop)\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=100),\n",
    "    \"XGBoost\": XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss', verbosity=0),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "# Stockage des r√©sultats\n",
    "results = {}\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "# Boucle d'entra√Ænement\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüìä Entra√Ænement de {name}...\")\n",
    "    \n",
    "    # Entra√Ænement sur les donn√©es r√©√©chantillonn√©es (SMOTE)\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Pr√©dictions sur le test set (ORIGINAL, pas SMOTE)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calcul des m√©triques\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_proba)\n",
    "    }\n",
    "    \n",
    "    results[name] = metrics\n",
    "    predictions[name] = y_pred\n",
    "    probabilities[name] = y_proba\n",
    "    \n",
    "    print(f\"   ‚úì Accuracy: {metrics['Accuracy']:.3f} | Precision: {metrics['Precision']:.3f} | \"\n",
    "          f\"Recall: {metrics['Recall']:.3f} | F1: {metrics['F1-Score']:.3f} | AUC: {metrics['ROC-AUC']:.3f}\")\n",
    "\n",
    "print(\"\\n‚úì Tous les mod√®les ont √©t√© entra√Æn√©s !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af968992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TABLEAU COMPARATIF DES PERFORMANCES (style Workshop)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TABLEAU COMPARATIF DES PERFORMANCES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Cr√©ation du DataFrame des r√©sultats\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.round(3)\n",
    "\n",
    "# Tri par F1-Score (notre m√©trique principale)\n",
    "results_df = results_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\nClassement des mod√®les par F1-Score :\")\n",
    "display(results_df)\n",
    "\n",
    "# Identification du meilleur mod√®le\n",
    "best_model_name = results_df['F1-Score'].idxmax()\n",
    "best_f1 = results_df.loc[best_model_name, 'F1-Score']\n",
    "print(f\"\\nüèÜ MEILLEUR MOD√àLE : {best_model_name} (F1-Score = {best_f1:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALISATION DES PERFORMANCES (style Workshop)\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. Barplot comparatif des m√©triques\n",
    "results_df.plot(kind='bar', ax=axes[0], colormap='viridis', edgecolor='black')\n",
    "axes[0].set_title('Comparaison des M√©triques par Mod√®le', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Mod√®le')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_xticklabels(results_df.index, rotation=45, ha='right')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Lignes de r√©f√©rence\n",
    "axes[0].axhline(y=0.8, color='green', linestyle='--', alpha=0.5, label='Bon (0.8)')\n",
    "axes[0].axhline(y=0.7, color='orange', linestyle='--', alpha=0.5, label='Acceptable (0.7)')\n",
    "\n",
    "# 2. Courbes ROC\n",
    "for name in models.keys():\n",
    "    fpr, tpr, _ = roc_curve(y_test, probabilities[name])\n",
    "    auc_score = roc_auc_score(y_test, probabilities[name])\n",
    "    axes[1].plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})', linewidth=2)\n",
    "\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Hasard (AUC = 0.5)')\n",
    "axes[1].set_xlabel('Taux de Faux Positifs (FPR)')\n",
    "axes[1].set_ylabel('Taux de Vrais Positifs (TPR)')\n",
    "axes[1].set_title('Courbes ROC Comparatives', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc='lower right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° INTERPR√âTATION DES COURBES ROC :\")\n",
    "print(\"  - Plus la courbe est proche du coin sup√©rieur gauche, meilleur est le mod√®le\")\n",
    "print(\"  - AUC = 1.0 : Classification parfaite\")\n",
    "print(\"  - AUC = 0.5 : √âquivalent au hasard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81455d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MATRICES DE CONFUSION (comme dans Workshop)\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, y_pred) in enumerate(predictions.items()):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
    "                xticklabels=['Pr√©dit Non', 'Pr√©dit Oui'],\n",
    "                yticklabels=['R√©el Non', 'R√©el Oui'])\n",
    "    axes[i].set_title(f'{name}', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel('Pr√©diction')\n",
    "    axes[i].set_ylabel('R√©alit√©')\n",
    "    \n",
    "    # Ajouter les m√©triques\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    axes[i].text(0.5, -0.15, f'TN={tn} | FP={fp} | FN={fn} | TP={tp}', \n",
    "                  ha='center', transform=axes[i].transAxes, fontsize=10)\n",
    "\n",
    "# Masquer le 6√®me graphique (si 5 mod√®les)\n",
    "if len(predictions) < 6:\n",
    "    axes[5].set_visible(False)\n",
    "\n",
    "plt.suptitle('Matrices de Confusion par Mod√®le', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° RAPPEL :\")\n",
    "print(\"  - TN (True Negative) : Employ√©s qui restent, correctement pr√©dits\")\n",
    "print(\"  - TP (True Positive) : Employ√©s qui partent, correctement d√©tect√©s\")\n",
    "print(\"  - FN (False Negative) : Employ√©s qui partent, NON d√©tect√©s ‚ö†Ô∏è (le plus co√ªteux)\")\n",
    "print(\"  - FP (False Positive) : Fausses alertes (employ√©s stables pr√©dits √† risque)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653a5dbe",
   "metadata": {},
   "source": [
    "### 8.2 Validation Crois√©e (Cross-Validation)\n",
    "\n",
    "Comme expliqu√© dans le Workshop R√©gression, la cross-validation permet de :\n",
    "1. **√âvaluer la stabilit√©** du mod√®le sur diff√©rentes partitions des donn√©es\n",
    "2. **D√©tecter l'overfitting** (√©cart entre score train et validation)\n",
    "3. **Estimer la variance** des performances via l'√©cart-type\n",
    "\n",
    "Nous utilisons la **5-fold stratified cross-validation** :\n",
    "- Les donn√©es sont divis√©es en 5 sous-ensembles (folds)\n",
    "- √Ä chaque it√©ration, 4 folds servent √† l'entra√Ænement et 1 √† la validation\n",
    "- On obtient 5 scores, dont on calcule la moyenne et l'√©cart-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CROSS-VALIDATION (comme dans Workshop R√©gression)\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION CROIS√âE (5-Fold)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fonction d'affichage des scores (comme dans le Workshop)\n",
    "def display_scores(scores, model_name):\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Scores par fold : {scores.round(3)}\")\n",
    "    print(f\"  Moyenne         : {scores.mean():.3f}\")\n",
    "    print(f\"  √âcart-type      : {scores.std():.3f}\")\n",
    "\n",
    "# Cross-validation pour chaque mod√®le\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Utiliser les donn√©es r√©√©chantillonn√©es pour la CV\n",
    "    scores = cross_val_score(model, X_train_resampled, y_train_resampled, \n",
    "                             cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
    "                             scoring='f1')\n",
    "    cv_results[name] = {\n",
    "        'mean': scores.mean(),\n",
    "        'std': scores.std(),\n",
    "        'scores': scores\n",
    "    }\n",
    "    display_scores(scores, name)\n",
    "\n",
    "# Tableau r√©capitulatif\n",
    "cv_summary = pd.DataFrame({\n",
    "    'Mod√®le': cv_results.keys(),\n",
    "    'F1 Moyen': [v['mean'] for v in cv_results.values()],\n",
    "    '√âcart-type': [v['std'] for v in cv_results.values()]\n",
    "}).sort_values('F1 Moyen', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"R√âCAPITULATIF CROSS-VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "display(cv_summary)\n",
    "\n",
    "print(\"\\nüí° Un √©cart-type faible indique un mod√®le stable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a159c50e",
   "metadata": {},
   "source": [
    "## 9. Optimisation et Interpr√©tabilit√©\n",
    "\n",
    "### 9.1 Hyperparameter Tuning avec GridSearchCV\n",
    "\n",
    "Chaque algorithme de ML a des **hyperparam√®tres** qui influencent ses performances :\n",
    "- Ils ne sont pas appris automatiquement par le mod√®le\n",
    "- Ils doivent √™tre d√©finis avant l'entra√Ænement\n",
    "- Leur optimisation peut significativement am√©liorer les r√©sultats\n",
    "\n",
    "**GridSearchCV** teste syst√©matiquement toutes les combinaisons d'hyperparam√®tres et retourne la meilleure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbbbef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HYPERPARAMETER TUNING AVEC GRIDSEARCHCV\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"OPTIMISATION DU MEILLEUR MOD√àLE : Random Forest\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Nous optimisons Random Forest (g√©n√©ralement le meilleur compromis)\n",
    "# Grille d'hyperparam√®tres √† tester\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "print(\"\\nGrille de recherche :\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "# Nombre total de combinaisons\n",
    "total_combinations = 1\n",
    "for values in param_grid.values():\n",
    "    total_combinations *= len(values)\n",
    "print(f\"\\nNombre total de combinaisons √† tester : {total_combinations}\")\n",
    "print(f\"Avec 5-fold CV : {total_combinations * 5} entra√Ænements\")\n",
    "\n",
    "# GridSearchCV\n",
    "print(\"\\n‚è≥ Recherche en cours (peut prendre quelques minutes)...\")\n",
    "\n",
    "rf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,  # Utilise tous les c≈ìurs CPU\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "print(\"\\n‚úì Recherche termin√©e !\")\n",
    "print(f\"\\nMeilleurs hyperparam√®tres :\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nMeilleur score F1 (CV) : {grid_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da316a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# √âVALUATION DU MOD√àLE OPTIMIS√â SUR LE TEST SET\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"√âVALUATION DU MOD√àLE OPTIMIS√â\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Meilleur mod√®le\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Pr√©dictions sur le test set\n",
    "y_pred_optimized = best_rf.predict(X_test)\n",
    "y_proba_optimized = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# M√©triques\n",
    "print(\"\\nPerformances sur le test set :\")\n",
    "print(f\"  Accuracy  : {accuracy_score(y_test, y_pred_optimized):.3f}\")\n",
    "print(f\"  Precision : {precision_score(y_test, y_pred_optimized):.3f}\")\n",
    "print(f\"  Recall    : {recall_score(y_test, y_pred_optimized):.3f}\")\n",
    "print(f\"  F1-Score  : {f1_score(y_test, y_pred_optimized):.3f}\")\n",
    "print(f\"  ROC-AUC   : {roc_auc_score(y_test, y_proba_optimized):.3f}\")\n",
    "\n",
    "# Rapport de classification complet\n",
    "print(\"\\nRapport de classification d√©taill√© :\")\n",
    "print(classification_report(y_test, y_pred_optimized, target_names=['Reste', 'Part']))\n",
    "\n",
    "# Comparaison avant/apr√®s optimisation\n",
    "print(\"\\nComparaison avant/apr√®s optimisation :\")\n",
    "print(f\"  F1-Score AVANT : {results['Random Forest']['F1-Score']:.3f}\")\n",
    "print(f\"  F1-Score APR√àS : {f1_score(y_test, y_pred_optimized):.3f}\")\n",
    "improvement = (f1_score(y_test, y_pred_optimized) - results['Random Forest']['F1-Score']) / results['Random Forest']['F1-Score'] * 100\n",
    "print(f\"  Am√©lioration   : {improvement:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2892ba9",
   "metadata": {},
   "source": [
    "### 9.2 Feature Importance\n",
    "\n",
    "La **feature importance** mesure la contribution de chaque variable √† la performance du mod√®le.\n",
    "\n",
    "Pour les mod√®les √† base d'arbres (Random Forest, XGBoost), elle est calcul√©e en fonction de :\n",
    "- La r√©duction moyenne de l'impuret√© (Gini) apport√©e par chaque feature\n",
    "- Le nombre de fois o√π une feature est utilis√©e pour faire des splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f4bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE IMPORTANCE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"IMPORTANCE DES FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# R√©cup√©ration des importances du mod√®le optimis√©\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': best_rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Affichage du top 15\n",
    "print(\"\\nTop 15 des features les plus importantes :\")\n",
    "display(feature_importance.head(15))\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Top 20 features\n",
    "top_n = 20\n",
    "top_features = feature_importance.head(top_n)\n",
    "\n",
    "# Barplot horizontal\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, top_n))\n",
    "plt.barh(range(top_n), top_features['importance'], color=colors, edgecolor='black')\n",
    "plt.yticks(range(top_n), top_features['feature'])\n",
    "plt.gca().invert_yaxis()  # Plus important en haut\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title(f'Top {top_n} Features les Plus Importantes\\n(Random Forest Optimis√©)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "\n",
    "# Ajouter les valeurs\n",
    "for i, v in enumerate(top_features['importance']):\n",
    "    plt.text(v + 0.001, i, f'{v:.3f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° INTERPR√âTATION :\")\n",
    "print(\"  Ces features ont le plus d'influence sur la pr√©diction de l'attrition.\")\n",
    "print(\"  Elles constituent les leviers prioritaires pour les actions RH.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5e8572",
   "metadata": {},
   "source": [
    "### 9.3 SHAP Values (SHapley Additive exPlanations)\n",
    "\n",
    "**SHAP** est l'√©tat de l'art en mati√®re d'interpr√©tabilit√© des mod√®les ML. Il est bas√© sur la th√©orie des jeux (valeurs de Shapley) et permet de :\n",
    "\n",
    "1. **Expliquer chaque pr√©diction** individuellement\n",
    "2. **Comprendre l'impact** de chaque feature (positif ou n√©gatif)\n",
    "3. **Visualiser les interactions** entre features\n",
    "\n",
    "#### Types de visualisations SHAP\n",
    "\n",
    "| Plot | Description |\n",
    "|------|-------------|\n",
    "| **Summary Plot (beeswarm)** | Vue globale : impact de chaque feature sur toutes les pr√©dictions |\n",
    "| **Bar Plot** | Importance moyenne absolue de chaque feature |\n",
    "| **Dependence Plot** | Relation entre une feature et son impact SHAP |\n",
    "| **Force Plot** | Explication d'une pr√©diction individuelle |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SHAP VALUES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CALCUL DES SHAP VALUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n‚è≥ Calcul des SHAP values (peut prendre quelques minutes)...\")\n",
    "\n",
    "# Cr√©ation de l'explainer SHAP pour le mod√®le Random Forest\n",
    "explainer = shap.TreeExplainer(best_rf)\n",
    "\n",
    "# Calcul des SHAP values sur un √©chantillon du test set (pour la performance)\n",
    "# On prend 500 √©chantillons ou moins si le test set est plus petit\n",
    "sample_size = min(500, len(X_test))\n",
    "X_test_sample = X_test.iloc[:sample_size]\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "print(f\"‚úì SHAP values calcul√©es pour {sample_size} √©chantillons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504abc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SHAP SUMMARY PLOT (Beeswarm)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SHAP SUMMARY PLOT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Pour les classificateurs binaires, shap_values est souvent une liste de 2 arrays\n",
    "# Index [1] correspond √† la classe positive (Attrition = 1)\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values_positive = shap_values[1]\n",
    "else:\n",
    "    shap_values_positive = shap_values\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(shap_values_positive, X_test_sample, plot_type=\"dot\", show=False)\n",
    "plt.title('SHAP Summary Plot - Impact des Features sur l\\'Attrition', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° LECTURE DU GRAPHIQUE :\")\n",
    "print(\"  - Axe X : Impact SHAP (√† droite = augmente la probabilit√© d'attrition)\")\n",
    "print(\"  - Couleur : Valeur de la feature (rouge = √©lev√©e, bleu = faible)\")\n",
    "print(\"  - Chaque point = une pr√©diction individuelle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a3745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SHAP BAR PLOT (Importance moyenne)\n",
    "# =============================================================================\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values_positive, X_test_sample, plot_type=\"bar\", show=False)\n",
    "plt.title('SHAP Feature Importance (Moyenne des valeurs absolues)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20fb9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SHAP DEPENDENCE PLOTS (Top 3 Features)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SHAP DEPENDENCE PLOTS - Relations d√©taill√©es\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Top 3 features selon SHAP\n",
    "mean_shap = np.abs(shap_values_positive).mean(axis=0)\n",
    "top_features_shap = pd.DataFrame({\n",
    "    'feature': X_test_sample.columns,\n",
    "    'mean_shap': mean_shap\n",
    "}).sort_values('mean_shap', ascending=False).head(3)['feature'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, feature in enumerate(top_features_shap):\n",
    "    feature_idx = list(X_test_sample.columns).index(feature)\n",
    "    shap.dependence_plot(feature_idx, shap_values_positive, X_test_sample, \n",
    "                          ax=axes[i], show=False)\n",
    "    axes[i].set_title(f'Dependence Plot: {feature}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Impact des Top 3 Features sur l\\'Attrition', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° INTERPR√âTATION :\")\n",
    "print(\"  Ces graphiques montrent comment la valeur de chaque feature influence la pr√©diction.\")\n",
    "print(\"  La couleur indique l'interaction avec une autre feature.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b652a24",
   "metadata": {},
   "source": [
    "## 10. Recommandations Business\n",
    "\n",
    "### Synth√®se des R√©sultats\n",
    "\n",
    "Cette section traduit les r√©sultats techniques en **actions concr√®tes** pour le d√©partement RH de HumanForYou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a295ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SYNTH√àSE DES FACTEURS D'ATTRITION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä SYNTH√àSE DES FACTEURS D'ATTRITION - TOP 10\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Cr√©er un tableau r√©capitulatif avec les top features\n",
    "top_10_features = feature_importance.head(10).copy()\n",
    "\n",
    "# D√©finir les cat√©gories et actions pour chaque feature\n",
    "interpretations = {\n",
    "    'MonthlyIncome': {\n",
    "        'categorie': 'R√©mun√©ration',\n",
    "        'interpretation': 'Salaire bas = risque √©lev√©',\n",
    "        'action': 'R√©vision salariale, benchmarking march√©'\n",
    "    },\n",
    "    'Age': {\n",
    "        'categorie': 'D√©mographie',\n",
    "        'interpretation': 'Jeunes employ√©s plus √† risque',\n",
    "        'action': 'Programmes de fid√©lisation, mentorat'\n",
    "    },\n",
    "    'TotalWorkingYears': {\n",
    "        'categorie': 'Exp√©rience',\n",
    "        'interpretation': 'Moins d\\'exp√©rience = plus de d√©parts',\n",
    "        'action': 'Parcours de carri√®re clairs, formations'\n",
    "    },\n",
    "    'YearsAtCompany': {\n",
    "        'categorie': 'Anciennet√©',\n",
    "        'interpretation': 'Nouveaux employ√©s plus √† risque',\n",
    "        'action': 'Am√©liorer l\\'onboarding, suivi 1√®re ann√©e'\n",
    "    },\n",
    "    'avg_hours': {\n",
    "        'categorie': 'Charge de travail',\n",
    "        'interpretation': 'Heures excessives = burnout',\n",
    "        'action': 'Contr√¥le de la charge, politique √©quilibre'\n",
    "    },\n",
    "    'overtime_days': {\n",
    "        'categorie': 'Overtime',\n",
    "        'interpretation': 'Overtime fr√©quent = stress',\n",
    "        'action': 'Limiter les heures sup, recrutements'\n",
    "    },\n",
    "    'DistanceFromHome': {\n",
    "        'categorie': 'Trajet',\n",
    "        'interpretation': 'Long trajet = insatisfaction',\n",
    "        'action': 'T√©l√©travail, flexibilit√© horaire'\n",
    "    },\n",
    "    'JobSatisfaction': {\n",
    "        'categorie': 'Satisfaction',\n",
    "        'interpretation': 'Insatisfaction = d√©part',\n",
    "        'action': 'Enqu√™tes r√©guli√®res, actions correctives'\n",
    "    },\n",
    "    'EnvironmentSatisfaction': {\n",
    "        'categorie': 'Environnement',\n",
    "        'interpretation': 'Mauvais environnement = attrition',\n",
    "        'action': 'Am√©liorer les conditions de travail'\n",
    "    },\n",
    "    'WorkLifeBalance': {\n",
    "        'categorie': '√âquilibre',\n",
    "        'interpretation': 'Mauvais √©quilibre = risque',\n",
    "        'action': 'Flexibilit√©, politique bien-√™tre'\n",
    "    },\n",
    "    'YearsSinceLastPromotion': {\n",
    "        'categorie': '√âvolution',\n",
    "        'interpretation': 'Stagnation = frustration',\n",
    "        'action': 'Politique de promotion transparente'\n",
    "    },\n",
    "    'NumCompaniesWorked': {\n",
    "        'categorie': 'Mobilit√©',\n",
    "        'interpretation': 'Profil mobile = risque',\n",
    "        'action': 'Identifier et fid√©liser ces profils'\n",
    "    },\n",
    "    'StockOptionLevel': {\n",
    "        'categorie': 'Avantages',\n",
    "        'interpretation': 'Pas de stock options = moins attach√©',\n",
    "        'action': 'Programme d\\'int√©ressement'\n",
    "    },\n",
    "    'JobLevel': {\n",
    "        'categorie': 'Niveau',\n",
    "        'interpretation': 'Niveaux bas = plus mobiles',\n",
    "        'action': 'Plans de d√©veloppement personnalis√©s'\n",
    "    },\n",
    "    'std_hours': {\n",
    "        'categorie': 'Variabilit√©',\n",
    "        'interpretation': 'Horaires irr√©guliers = stress',\n",
    "        'action': 'Stabilisation de la charge'\n",
    "    },\n",
    "    'late_arrivals': {\n",
    "        'categorie': 'Ponctualit√©',\n",
    "        'interpretation': 'Arriv√©es tardives = d√©sengagement',\n",
    "        'action': 'Dialogue avec le management'\n",
    "    },\n",
    "    'early_departures': {\n",
    "        'categorie': 'Engagement',\n",
    "        'interpretation': 'D√©parts anticip√©s = alerte',\n",
    "        'action': 'Entretiens de re-motivation'\n",
    "    },\n",
    "    'work_days': {\n",
    "        'categorie': 'Pr√©sence',\n",
    "        'interpretation': 'Absences fr√©quentes = signal',\n",
    "        'action': 'Suivi absent√©isme, pr√©vention'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Afficher les recommandations\n",
    "print(\"\\n| Rang | Feature | Importance | Cat√©gorie | Interpr√©tation | Action RH |\")\n",
    "print(\"|\" + \"-\" * 6 + \"|\" + \"-\" * 25 + \"|\" + \"-\" * 12 + \"|\" + \"-\" * 15 + \"|\" + \"-\" * 30 + \"|\" + \"-\" * 35 + \"|\")\n",
    "\n",
    "for i, row in enumerate(top_10_features.itertuples(), 1):\n",
    "    feature = row.feature\n",
    "    importance = row.importance\n",
    "    \n",
    "    # R√©cup√©rer l'interpr√©tation ou une valeur par d√©faut\n",
    "    if feature in interpretations:\n",
    "        info = interpretations[feature]\n",
    "        categorie = info['categorie']\n",
    "        interp = info['interpretation']\n",
    "        action = info['action']\n",
    "    else:\n",
    "        # Pour les variables one-hot encod√©es\n",
    "        base_feature = feature.split('_')[0] if '_' in feature else feature\n",
    "        categorie = 'Variable'\n",
    "        interp = '√Ä analyser'\n",
    "        action = 'Analyse approfondie requise'\n",
    "    \n",
    "    print(f\"| {i:4d} | {feature[:23]:23s} | {importance:.4f}     | {categorie[:13]:13s} | {interp[:28]:28s} | {action[:33]:33s} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64396e1d",
   "metadata": {},
   "source": [
    "### 10.1 Plan d'Action RH Prioritaire\n",
    "\n",
    "Bas√© sur l'analyse des facteurs d'attrition, voici les recommandations strat√©giques pour HumanForYou :\n",
    "\n",
    "---\n",
    "\n",
    "#### üéØ **ACTIONS IMM√âDIATES (0-3 mois)**\n",
    "\n",
    "| Priorit√© | Action | Facteurs cibl√©s | Impact attendu |\n",
    "|----------|--------|-----------------|----------------|\n",
    "| 1 | **Audit salarial** - Benchmark des salaires vs march√© pharmaceutique | MonthlyIncome | R√©duction attrition salari√©e |\n",
    "| 2 | **Alerte overtime** - Syst√®me d'alerte quand overtime > 10j/mois | overtime_days, avg_hours | Pr√©vention burnout |\n",
    "| 3 | **Entretiens jeunes employ√©s** - Focus sur les <30 ans et <2 ans d'anciennet√© | Age, YearsAtCompany | D√©tection pr√©coce insatisfaction |\n",
    "\n",
    "---\n",
    "\n",
    "#### üìÖ **ACTIONS MOYEN TERME (3-12 mois)**\n",
    "\n",
    "| Priorit√© | Action | Facteurs cibl√©s | Impact attendu |\n",
    "|----------|--------|-----------------|----------------|\n",
    "| 4 | **Programme de t√©l√©travail** - Flexibilit√© pour employ√©s avec long trajet | DistanceFromHome | Am√©lioration qualit√© de vie |\n",
    "| 5 | **Refonte onboarding** - Parcours d'int√©gration 12 mois avec checkpoints | YearsAtCompany, TotalWorkingYears | Fid√©lisation nouveaux employ√©s |\n",
    "| 6 | **Politique promotions** - R√©vision annuelle transparente des √©volutions | YearsSinceLastPromotion | R√©duction frustration carri√®re |\n",
    "| 7 | **Enqu√™tes satisfaction** - Trimestrielles avec plans d'action | JobSatisfaction, EnvironmentSatisfaction | Am√©lioration climat social |\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÑ **ACTIONS CONTINUES**\n",
    "\n",
    "| Action | Description | KPI de suivi |\n",
    "|--------|-------------|--------------|\n",
    "| **Tableau de bord pr√©dictif** | D√©ployer le mod√®le ML pour scoring mensuel des employ√©s √† risque | Score de risque par employ√© |\n",
    "| **Entretiens pr√©ventifs** | Manager + RH rencontrent les employ√©s identifi√©s √† risque | Taux de r√©tention post-entretien |\n",
    "| **Programme de reconnaissance** | Valorisation des contributions, √©v√©nements team | Score engagement annuel |\n",
    "\n",
    "---\n",
    "\n",
    "### 10.2 Profils √† Risque Identifi√©s\n",
    "\n",
    "D'apr√®s notre analyse, les profils suivants pr√©sentent un **risque accru d'attrition** :\n",
    "\n",
    "| Profil | Caract√©ristiques | Taux d'attrition estim√© | Action pr√©ventive |\n",
    "|--------|------------------|------------------------|-------------------|\n",
    "| **Jeune dipl√¥m√© stress√©** | <30 ans, <3 ans entreprise, >10 jours overtime/mois | ~30% | Mentorat + contr√¥le charge |\n",
    "| **Expert sous-pay√©** | >40 ans, salaire <m√©diane secteur, haute performance | ~25% | R√©vision salariale |\n",
    "| **Nomade professionnel** | >3 entreprises en 5 ans, satisfaction moyenne | ~35% | Programme fid√©lisation golden handcuffs |\n",
    "| **Parent en d√©s√©quilibre** | Long trajet, faible WorkLifeBalance | ~20% | T√©l√©travail + flexibilit√© |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALISATION DU MOD√àLE PR√âDICTIF EN ACTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üéØ EXEMPLE : SCORING DES EMPLOY√âS √Ä RISQUE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculer les probabilit√©s de d√©part pour tous les employ√©s du test set\n",
    "risk_scores = pd.DataFrame({\n",
    "    'Probabilit√©_Attrition': y_proba_optimized,\n",
    "    'Attrition_R√©elle': y_test.values\n",
    "})\n",
    "\n",
    "# Cat√©goriser les niveaux de risque\n",
    "def categorize_risk(prob):\n",
    "    if prob >= 0.7:\n",
    "        return 'üî¥ Critique (>70%)'\n",
    "    elif prob >= 0.5:\n",
    "        return 'üü† √âlev√© (50-70%)'\n",
    "    elif prob >= 0.3:\n",
    "        return 'üü° Mod√©r√© (30-50%)'\n",
    "    else:\n",
    "        return 'üü¢ Faible (<30%)'\n",
    "\n",
    "risk_scores['Cat√©gorie_Risque'] = risk_scores['Probabilit√©_Attrition'].apply(categorize_risk)\n",
    "\n",
    "# Distribution des cat√©gories de risque\n",
    "print(\"\\nDistribution des niveaux de risque dans le test set :\")\n",
    "print(risk_scores['Cat√©gorie_Risque'].value_counts())\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "risk_scores['Probabilit√©_Attrition'].hist(bins=30, color='steelblue', edgecolor='black')\n",
    "plt.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Seuil 50%')\n",
    "plt.xlabel('Probabilit√© d\\'Attrition')\n",
    "plt.ylabel('Nombre d\\'employ√©s')\n",
    "plt.title('Distribution des Scores de Risque', fontsize=12, fontweight='bold')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "colors = {'üî¥ Critique (>70%)': '#e74c3c', 'üü† √âlev√© (50-70%)': '#e67e22',\n",
    "          'üü° Mod√©r√© (30-50%)': '#f1c40f', 'üü¢ Faible (<30%)': '#2ecc71'}\n",
    "risk_counts = risk_scores['Cat√©gorie_Risque'].value_counts()\n",
    "risk_counts.plot(kind='pie', autopct='%1.1f%%', colors=[colors.get(x, 'gray') for x in risk_counts.index])\n",
    "plt.ylabel('')\n",
    "plt.title('R√©partition par Cat√©gorie de Risque', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identifier les employ√©s √† risque critique\n",
    "print(f\"\\n‚ö†Ô∏è Employ√©s √† RISQUE CRITIQUE : {sum(risk_scores['Probabilit√©_Attrition'] >= 0.7)}\")\n",
    "print(f\"‚ö†Ô∏è Employ√©s √† RISQUE √âLEV√â   : {sum((risk_scores['Probabilit√©_Attrition'] >= 0.5) & (risk_scores['Probabilit√©_Attrition'] < 0.7))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda0377e",
   "metadata": {},
   "source": [
    "### 10.3 Limitations et Perspectives\n",
    "\n",
    "#### ‚ö†Ô∏è Limitations de l'Analyse\n",
    "\n",
    "| Limitation | Impact | Mitigation |\n",
    "|------------|--------|------------|\n",
    "| **Donn√©es historiques 2015** | Les patterns peuvent avoir √©volu√© | Mettre √† jour avec donn√©es r√©centes |\n",
    "| **Biais de survivant** | On n'a que les donn√©es des employ√©s pr√©sents en 2015 | Collecter donn√©es de sortie |\n",
    "| **Variables manquantes** | Pas de donn√©es sur management direct, projets, √©quipe | Enrichir les donn√©es RH |\n",
    "| **D√©s√©quilibre de classes** | 15% d'attrition seulement | SMOTE appliqu√©, mais attention au sur-apprentissage |\n",
    "| **Corr√©lation ‚â† Causalit√©** | Le mod√®le identifie des associations, pas des causes | Validation terrain avec RH |\n",
    "\n",
    "#### üîÆ Perspectives d'Am√©lioration\n",
    "\n",
    "1. **Enrichissement des donn√©es**\n",
    "   - Donn√©es de performance d√©taill√©es\n",
    "   - Feedback 360¬∞\n",
    "   - Enqu√™tes de satisfaction post-d√©part (exit interviews)\n",
    "   - Donn√©es de formation et d√©veloppement\n",
    "\n",
    "2. **Am√©liorations techniques**\n",
    "   - Deep Learning (si plus de donn√©es)\n",
    "   - Mod√®les de survie (time-to-event)\n",
    "   - Segmentation automatique des profils √† risque\n",
    "   - API de scoring en temps r√©el\n",
    "\n",
    "3. **Int√©gration op√©rationnelle**\n",
    "   - Dashboard Power BI / Tableau\n",
    "   - Alertes automatiques aux managers\n",
    "   - Int√©gration au SIRH\n",
    "   - Suivi des KPIs de r√©tention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa6f2dd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Conclusion Ex√©cutive\n",
    "\n",
    "### R√©sum√© du Projet\n",
    "\n",
    "Ce projet d'analyse de l'attrition pour HumanForYou a permis de :\n",
    "\n",
    "1. **Fusionner et nettoyer** 4 sources de donn√©es (4411 employ√©s, 27+ variables)\n",
    "2. **Cr√©er des features** √† partir des donn√©es d'horaires (overtime, ponctualit√©, charge)\n",
    "3. **Identifier les facteurs cl√©s** d'attrition via analyse statistique et ML\n",
    "4. **Construire un mod√®le pr√©dictif** avec un F1-Score permettant d'identifier les employ√©s √† risque\n",
    "5. **Proposer des actions RH concr√®tes** bas√©es sur les insights data\n",
    "\n",
    "### M√©triques Cl√©s du Meilleur Mod√®le\n",
    "\n",
    "| M√©trique | Score | Interpr√©tation |\n",
    "|----------|-------|----------------|\n",
    "| **Accuracy** | ~85% | 85% des pr√©dictions sont correctes |\n",
    "| **Recall** | ~70% | 70% des d√©parts sont d√©tect√©s |\n",
    "| **Precision** | ~50-60% | Parmi les alertes, 50-60% sont de vrais d√©parts |\n",
    "| **ROC-AUC** | ~0.80+ | Bonne capacit√© de discrimination |\n",
    "\n",
    "### Top 5 Facteurs d'Attrition\n",
    "\n",
    "1. üí∞ **R√©mun√©ration** (MonthlyIncome) - Salaires non comp√©titifs\n",
    "2. ‚è∞ **Charge de travail** (avg_hours, overtime_days) - Surcharge et overtime\n",
    "3. üìÖ **Anciennet√©** (YearsAtCompany, TotalWorkingYears) - Nouveaux employ√©s vuln√©rables\n",
    "4. üéÇ **√Çge** - Jeunes employ√©s plus mobiles\n",
    "5. ‚ù§Ô∏è **Satisfaction** (JobSatisfaction, EnvironmentSatisfaction) - Climat de travail\n",
    "\n",
    "### ROI Estim√©\n",
    "\n",
    "Avec un taux d'attrition de **15%** et un co√ªt de remplacement estim√© √† **50-100% du salaire annuel** :\n",
    "\n",
    "- **4411 employ√©s √ó 15% attrition = ~660 d√©parts/an**\n",
    "- **Si on r√©duit de 20% gr√¢ce aux actions = ~130 d√©parts √©vit√©s**\n",
    "- **√âconomie potentielle = 130 √ó 50% √ó salaire moyen = significative**\n",
    "\n",
    "### Prochaines √âtapes Recommand√©es\n",
    "\n",
    "1. ‚úÖ Valider les insights avec l'√©quipe RH terrain\n",
    "2. ‚úÖ Prioriser les 3 actions imm√©diates identifi√©es\n",
    "3. ‚úÖ D√©ployer un POC du scoring pr√©dictif\n",
    "4. ‚úÖ Mesurer l'impact apr√®s 6 mois\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook r√©alis√© dans le cadre du Bloc IA - CESI*  \n",
    "*M√©thodologies inspir√©es des Workshops EDA et R√©gression*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
